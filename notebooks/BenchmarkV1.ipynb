{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lstm_bilstm\n",
    "from Utils.WordVecs import *\n",
    "from Utils.MyMetrics import *\n",
    "from Utils.Datasets import *\n",
    "from Utils.Semeval_2013_Dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"semeval\"\n",
    "bi = True\n",
    "dataset_raw = lstm_bilstm.Semeval_Dataset('../datasets/semeval',\n",
    "                                                None, rep=words,\n",
    "                                                one_hot=True)\n",
    "dataset = lstm_bilstm.Semeval_Dataset('../datasets/semeval',\n",
    "                                                None, rep=words,\n",
    "                                                one_hot=True)\n",
    "\n",
    "vecs = WordVecs('../embeddings/wiki.multi.en.vec', 'word2vec')\n",
    "dim = vecs.vector_size\n",
    "max_length = 0\n",
    "vocab = {}\n",
    "for sent in list(dataset._Xtrain) + list(dataset._Xdev) + list(dataset._Xtest):\n",
    "    if len(sent) > max_length:\n",
    "        max_length = len(sent)\n",
    "    for w in sent:\n",
    "        if w not in vocab:\n",
    "            vocab[w] = 1\n",
    "        else:\n",
    "            vocab[w] += 1\n",
    "            \n",
    "wordvecs = {}\n",
    "for w in vecs._w2idx.keys():\n",
    "    if w in vocab:\n",
    "        wordvecs[w] = vecs[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_bilstm.add_unknown_words(wordvecs, vocab, min_df=1, dim=dim)\n",
    "W, word_idx_map = lstm_bilstm.get_W(wordvecs, dim=dim)\n",
    "\n",
    "dataset = lstm_bilstm.convert_dataset(dataset, word_idx_map, max_length)\n",
    "dev_params_file = '../dev_params/300_bilstm.dev.txt'\n",
    "best_dim, best_dropout, best_epoch, best_f1 = lstm_bilstm.get_dev_params(name, dev_params_file, bi,\n",
    "                   dataset._Xtrain, dataset._ytrain, dataset._Xdev, dataset._ydev, wordvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../models/bilstm/'+ name +'/run1'\n",
    "best_weights = \"weights.006-0.6337.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lstm_bilstm.load_model(os.path.join(base_dir, best_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(dataset._Xtest, verbose=1)\n",
    "labels = sorted(set(dataset._ytrain.argmax(1)))\n",
    "mm = MyMetrics(dataset._ytest, pred, labels=labels, average='micro')\n",
    "acc, precision, recall, micro_f1 = mm.get_scores()\n",
    "print(micro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero shot benchmark\n",
    "- Load data from other language (tweets)\n",
    "- Tokenize data\n",
    "- load embeddings for that language\n",
    "- run model on other language with no training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading french data \n",
    "french_data = \"../../datasets/sentiment/french/French-Sentiment-Analysis-Dataset/tweets.csv\"\n",
    "labels = []\n",
    "tweets = []\n",
    "with open(french_data) as handle:\n",
    "    i = 0\n",
    "    for line in handle.readlines():\n",
    "        i += 1\n",
    "        if i == 1:\n",
    "            continue\n",
    "        try:\n",
    "            label = int(line[0])\n",
    "        except:\n",
    "            continue\n",
    "        text = line[2:]\n",
    "        tweets.append(text)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample 500 from each class of 0, 2 and 4\n",
    "test_tweets = []\n",
    "test_labels = []\n",
    "zero = 0\n",
    "two = 0\n",
    "four = 0\n",
    "max_count = 500\n",
    "for l, t in zip(labels, tweets):\n",
    "    if zero < max_count and l == 0:\n",
    "        zero += 1\n",
    "        test_tweets.append(t)\n",
    "        test_labels.append(l)\n",
    "    if two < max_count and l == 2:\n",
    "        two += 1\n",
    "        test_tweets.append(t)\n",
    "        test_labels.append(l)\n",
    "    if four < max_count and l == 4:\n",
    "        four += 1\n",
    "        test_tweets.append(t)\n",
    "        test_labels.append(l)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "import spacy\n",
    "fr_nlp = spacy.load('fr')\n",
    "test_tweets = [fr_nlp(t) for t in test_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "fr_file = '../../embeddings/wiki.multi.fr.vec'\n",
    "it_file = '../../embeddings/wiki.multi.it.vec'\n",
    "lang_files = [fr_file, it_file]\n",
    "\n",
    "embeddings = {}\n",
    "for lang_f in lang_files:\n",
    "    lang = lang_f[-6:-4]\n",
    "    embeddings[lang] = {}\n",
    "    with open(lang_f, 'r') as handle:\n",
    "        csv_file = csv.reader(handle, delimiter=' ', quotechar=\"|\")\n",
    "        i = 0\n",
    "        for row in csv_file:\n",
    "            if len(row) != 301:\n",
    "                continue\n",
    "            word = row[0]\n",
    "            vec = np.array(row[1:]).astype(np.float)\n",
    "            embeddings[lang][word] = vec\n",
    "            i += 1\n",
    "            if i % 50000 == 0:\n",
    "                print(i/2000., \"% complete ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunately we only have embeddings for 76% of the tokens\n",
    "found = []\n",
    "for t in test_tweets:\n",
    "    for tok in t:\n",
    "        if tok.text in embeddings['fr']:\n",
    "            found.append(1)\n",
    "        else:\n",
    "            found.append(0)\n",
    "print (np.mean(found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding weights with vocab and zeros\n",
    "fr_vocab = set()\n",
    "for sent in test_tweets:\n",
    "    for word in sent:\n",
    "        if word.text in embeddings['fr']:\n",
    "            fr_vocab.add(word.text)\n",
    "        \n",
    "# replace embedding in model\n",
    "fr_embeddings = np.zeros_like(clf.layers[0].get_weights()[0])\n",
    "fr_word_2_index = {}\n",
    "for i, word in enumerate(fr_vocab):\n",
    "    fr_word_2_index[word] = i+1\n",
    "    fr_embeddings[i+1] = embeddings['fr'][word]\n",
    "\n",
    "    \n",
    "# encode sentences with new index\n",
    "clf_fr = lstm_bilstm.load_model(os.path.join(base_dir, best_weights))\n",
    "clf_fr.layers[0].set_weights([fr_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sent(sent, word_idx_map, max_length=57):\n",
    "    encoded = np.array([word_idx_map[w.text] for w in sent if w.text in word_idx_map])\n",
    "    return encoded\n",
    "\n",
    "test_data = []\n",
    "for sent in test_tweets:\n",
    "    test_data.append(encode_sent(sent, fr_word_2_index))\n",
    "test_data = lstm_bilstm.pad_sequences(test_data, max_length)\n",
    "    \n",
    "pred = clf_fr.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.zeros_like(pred)\n",
    "for i, l in enumerate(test_labels):\n",
    "    pos = int(l/2)\n",
    "    true_labels[i][pos] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(set(dataset._ytrain.argmax(1)))\n",
    "mm = MyMetrics(true_labels, pred, labels=labels, average='micro')\n",
    "acc, precision, recall, micro_f1 = mm.get_scores()\n",
    "print(micro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_labels = np.zeros_like(pred)\n",
    "for i, x in enumerate(test_labels):\n",
    "    l = 4 if random.random() > .5 else 0\n",
    "    random_labels[i][int(l/2)] = 1.\n",
    "labels = sorted(set(dataset._ytrain.argmax(1)))\n",
    "mm = MyMetrics(random_labels, pred, labels=labels, average='micro')\n",
    "acc, precision, recall, micro_f1 = mm.get_scores()\n",
    "print(micro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(set(dataset._ytrain.argmax(1)))\n",
    "mm = MyMetrics(true_labels[:,[0,2]], pred[:,[0,2]], labels=labels, average='binary')\n",
    "acc, precision, recall, micro_f1 = mm.get_scores()\n",
    "print(acc, precision, recall, micro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "On French tweets we achieve __.36 F1__. It is low because we trained 3 classes (Positive, negative, neutral) but tested on only 2 classes (Positive, negative).  \n",
    "If we use take the argmax of only the positive and negative classes, we acheive __.67 F1__.  \n",
    "The results on the english dataset was also __.67 F1__.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading french data \n",
    "italian_data = \"../../datasets/sentiment/italian/test_set_sentipolc16_gold2000.csv\"\n",
    "test_labels = []\n",
    "tweets = []\n",
    "with open(italian_data) as handle:\n",
    "    i = 0\n",
    "    reader = csv.reader(handle)\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        text = row[8]\n",
    "        label_p = int(row[2])\n",
    "        label_n = int(row[3])\n",
    "        label = 1\n",
    "        if label_p == 1:\n",
    "            label = 2\n",
    "        if label_n == 1:\n",
    "            label = 0\n",
    "        if label_p + label_n == 2:\n",
    "            continue\n",
    "        tweets.append(text)\n",
    "        test_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "it_nlp = spacy.load('it')\n",
    "test_tweets = [it_nlp(t) for t in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = []\n",
    "for t in test_tweets:\n",
    "    for tok in t:\n",
    "        if tok.text in embeddings['it']:\n",
    "            found.append(1)\n",
    "        else:\n",
    "            found.append(0)\n",
    "print (np.mean(found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding weights with vocab and zeros\n",
    "it_vocab = set()\n",
    "for sent in test_tweets:\n",
    "    for word in sent:\n",
    "        if word.text in embeddings['it']:\n",
    "            it_vocab.add(word.text)\n",
    "        \n",
    "# replace embedding in model\n",
    "it_embeddings = np.zeros_like(clf.layers[0].get_weights()[0])\n",
    "it_word_2_index = {}\n",
    "for i, word in enumerate(it_vocab):\n",
    "    it_word_2_index[word] = i+1\n",
    "    it_embeddings[i+1] = embeddings['it'][word]\n",
    "\n",
    "    \n",
    "# encode sentences with new index\n",
    "clf_it = lstm_bilstm.load_model(os.path.join(base_dir, best_weights))\n",
    "clf_it.layers[0].set_weights([it_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sent(sent, word_idx_map, max_length=57):\n",
    "    encoded = np.array([word_idx_map[w.text] for w in sent if w.text in word_idx_map])\n",
    "    return encoded\n",
    "\n",
    "test_data = []\n",
    "for sent in test_tweets:\n",
    "    test_data.append(encode_sent(sent, it_word_2_index))\n",
    "test_data = lstm_bilstm.pad_sequences(test_data, max_length)\n",
    "    \n",
    "pred = clf_it.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.zeros_like(pred)\n",
    "for i, l in enumerate(test_labels):\n",
    "    true_labels[i][l] = 1.\n",
    "labels = sorted(set(dataset._ytrain.argmax(1)))\n",
    "mm = MyMetrics(true_labels, pred, labels=labels, average='micro')\n",
    "acc, precision, recall, micro_f1 = mm.get_scores()\n",
    "print(micro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Italian Results\n",
    "In Italian we acheive __.51 F1__ on 3 class classification.  \n",
    "Only 69% of tokens are in vocabulary for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
