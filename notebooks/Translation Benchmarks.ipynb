{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Benchmarks\n",
    "We will compare the previous results to:  \n",
    "(1) Training on the original data  \n",
    "(2) Evaluating the translations  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate translations for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'key':'trnsl.1.1.20180425T012837Z.34aff5ff1fbca7f9.449ba9cd74e4c4e829e77e60507b69d30b425d56',\n",
    "    'text':['hello world', 'this is great', 'one more'],\n",
    "    'lang':'es'\n",
    "}\n",
    "r = requests.post('https://translate.yandex.net/api/v1.5/tr.json/translate', data=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hola mundo', 'esta es una gran', 'uno de los m√°s']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.json()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading french data \n",
    "french_data = \"../../datasets/sentiment/french/French-Sentiment-Analysis-Dataset/tweets.csv\"\n",
    "labels = []\n",
    "tweets = []\n",
    "with open(french_data) as handle:\n",
    "    i = 0\n",
    "    for line in handle.readlines():\n",
    "        i += 1\n",
    "        if i == 1:\n",
    "            continue\n",
    "        try:\n",
    "            label = int(line[0])\n",
    "        except:\n",
    "            continue\n",
    "        text = line[2:]\n",
    "        tweets.append(text)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample 500 from each class of 0, 2 and 4\n",
    "test_tweets = []\n",
    "test_labels = []\n",
    "zero = 0\n",
    "two = 0\n",
    "four = 0\n",
    "max_count = 500\n",
    "for l, t in zip(labels, tweets):\n",
    "    if zero < max_count and l == 0:\n",
    "        zero += 1\n",
    "        test_tweets.append(t)\n",
    "        test_labels.append(l)\n",
    "    if two < max_count and l == 2:\n",
    "        two += 1\n",
    "        test_tweets.append(t)\n",
    "        test_labels.append(l)\n",
    "    if four < max_count and l == 4:\n",
    "        four += 1\n",
    "        test_tweets.append(t)\n",
    "        test_labels.append(l)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.839"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(t) for t in test_tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading italian data \n",
    "italian_data = \"../../datasets/sentiment/italian/test_set_sentipolc16_gold2000.csv\"\n",
    "test_labels = []\n",
    "tweets = []\n",
    "with open(italian_data) as handle:\n",
    "    i = 0\n",
    "    reader = csv.reader(handle)\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        text = row[8]\n",
    "        label_p = int(row[2])\n",
    "        label_n = int(row[3])\n",
    "        label = 1\n",
    "        if label_p == 1:\n",
    "            label = 2\n",
    "        if label_n == 1:\n",
    "            label = 0\n",
    "        if label_p + label_n == 2:\n",
    "            continue\n",
    "        tweets.append(text)\n",
    "        test_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109.70438328236493"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(t) for t in tweets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and store all translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translation(text_list, lang):\n",
    "    params = {\n",
    "        'key':'trnsl.1.1.20180425T012837Z.34aff5ff1fbca7f9.449ba9cd74e4c4e829e77e60507b69d30b425d56',\n",
    "        'text':text_list,\n",
    "        'lang':lang\n",
    "    }\n",
    "    r = requests.post('https://translate.yandex.net/api/v1.5/tr.json/translate', data=params)\n",
    "    return r.json()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# req_size = 50\n",
    "# i = 0 \n",
    "# french_translations = []\n",
    "# while i < len(test_tweets):\n",
    "#     end = i + req_size\n",
    "#     if end > len(test_tweets):\n",
    "#         end = len(test_tweets)\n",
    "#     trans = get_translation(test_tweets[i:end], 'fr-en')\n",
    "#     french_translations += trans\n",
    "#     i += req_size\n",
    "\n",
    "\n",
    "# req_size = 50\n",
    "# i = 0 \n",
    "# italian_translations = []\n",
    "# while i < len(tweets):\n",
    "#     end = i + req_size\n",
    "#     if end > len(tweets):\n",
    "#         end = len(tweets)\n",
    "#     trans = get_translation(tweets[i:end], 'it-en')\n",
    "#     italian_translations += trans\n",
    "#     i += req_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"french_trans.pkl\", 'wb') as handle:\n",
    "#     pickle.dump(french_translations, handle)\n",
    "    \n",
    "# with open(\"it_trans.pkl\", 'wb') as handle:\n",
    "#     pickle.dump(italian_translations, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(french_translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1962"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(italian_translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You do not see the hour that is here, at least, I will miss you more often and I will finally be able to be with him, in the face of the one who has friendzonato.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italian_translations[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# French translation eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "test_tweets = [nlp(t) for t in french_translations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0 % complete ...\n",
      "50.0 % complete ...\n",
      "75.0 % complete ...\n",
      "100.0 % complete ...\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "#fr_file = '../../embeddings/wiki.multi.fr.vec'\n",
    "#it_file = '../../embeddings/wiki.multi.it.vec'\n",
    "en_file = '../../embeddings/wiki.multi.en.vec'\n",
    "lang_files = [en_file]\n",
    "\n",
    "embeddings = {}\n",
    "for lang_f in lang_files:\n",
    "    lang = lang_f[-6:-4]\n",
    "    embeddings[lang] = {}\n",
    "    with open(lang_f, 'r') as handle:\n",
    "        csv_file = csv.reader(handle, delimiter=' ', quotechar=\"|\")\n",
    "        i = 0\n",
    "        for row in csv_file:\n",
    "            if len(row) != 301:\n",
    "                continue\n",
    "            word = row[0]\n",
    "            vec = np.array(row[1:]).astype(np.float)\n",
    "            embeddings[lang][word] = vec\n",
    "            i += 1\n",
    "            if i % 50000 == 0:\n",
    "                print(i/2000., \"% complete ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7635722812802\n"
     ]
    }
   ],
   "source": [
    "# Unfortunately we only have embeddings for 76% of the tokens\n",
    "found = []\n",
    "for t in test_tweets:\n",
    "    for tok in t:\n",
    "        if tok.text in embeddings['en']:\n",
    "            found.append(1)\n",
    "        else:\n",
    "            found.append(0)\n",
    "print (np.mean(found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lstm_bilstm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"semeval\"\n",
    "base_dir = '../models/bilstm/'+ name +'/run1'\n",
    "best_weights = \"weights.006-0.6337.hdf5\"\n",
    "clf = lstm_bilstm.load_model(os.path.join(base_dir, best_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding weights with vocab and zeros\n",
    "vocab = set()\n",
    "for sent in test_tweets:\n",
    "    for word in sent:\n",
    "        if word.text in embeddings['en']:\n",
    "            vocab.add(word.text)\n",
    "        \n",
    "# replace embedding in model\n",
    "en_embeddings = np.zeros_like(clf.layers[0].get_weights()[0])\n",
    "word_2_index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word_2_index[word] = i+1\n",
    "    en_embeddings[i+1] = embeddings['en'][word]\n",
    "\n",
    "    \n",
    "# encode sentences with new index\n",
    "clf_fr = lstm_bilstm.load_model(os.path.join(base_dir, best_weights))\n",
    "clf_fr.layers[0].set_weights([en_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.WordVecs import *\n",
    "from Utils.MyMetrics import *\n",
    "from Utils.Datasets import *\n",
    "from Utils.Semeval_2013_Dataset import *\n",
    "dataset = lstm_bilstm.Semeval_Dataset('../datasets/semeval',\n",
    "                                                None, rep=words,\n",
    "                                                one_hot=True)\n",
    "max_length = 0\n",
    "for sent in list(dataset._Xtrain) + list(dataset._Xdev) + list(dataset._Xtest):\n",
    "    if len(sent) > max_length:\n",
    "        max_length = len(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sent(sent, word_idx_map, max_length=57):\n",
    "    encoded = np.array([word_idx_map[w.text] for w in sent if w.text in word_idx_map])\n",
    "    return encoded\n",
    "\n",
    "test_data = []\n",
    "for sent in test_tweets:\n",
    "    test_data.append(encode_sent(sent, word_2_index))\n",
    "test_data = lstm_bilstm.pad_sequences(test_data, max_length)\n",
    "    \n",
    "pred = clf_fr.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.zeros_like(pred)\n",
    "for i, l in enumerate(test_labels):\n",
    "    pos = int(l/2)\n",
    "    true_labels[i][pos] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.373\n"
     ]
    }
   ],
   "source": [
    "# labels = sorted(set(dataset._ytrain.argmax(1)))\n",
    "# mm = MyMetrics(true_labels, pred, labels=labels, average='micro')\n",
    "# acc, precision, recall, micro_f1 = mm.get_scores()\n",
    "# print(micro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.616 0.5845481049562682 0.802 0.6762225969645869\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(set(dataset._ytrain.argmax(1)))\n",
    "mm = MyMetrics(true_labels[:,[0,2]], pred[:,[0,2]], labels=labels, average='binary')\n",
    "acc, precision, recall, micro_f1 = mm.get_scores()\n",
    "print(acc, precision, recall, micro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading french data \n",
    "italian_data = \"../../datasets/sentiment/italian/test_set_sentipolc16_gold2000.csv\"\n",
    "test_labels = []\n",
    "tweets = []\n",
    "with open(italian_data) as handle:\n",
    "    i = 0\n",
    "    reader = csv.reader(handle)\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        text = row[8]\n",
    "        label_p = int(row[2])\n",
    "        label_n = int(row[3])\n",
    "        label = 1\n",
    "        if label_p == 1:\n",
    "            label = 2\n",
    "        if label_n == 1:\n",
    "            label = 0\n",
    "        if label_p + label_n == 2:\n",
    "            continue\n",
    "        tweets.append(text)\n",
    "        test_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "733"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(test_labels) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "it_nlp = spacy.load('en')\n",
    "test_tweets = [it_nlp(t) for t in italian_translations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.723447680351676\n"
     ]
    }
   ],
   "source": [
    "found = []\n",
    "for t in test_tweets:\n",
    "    for tok in t:\n",
    "        if tok.text in embeddings['en']:\n",
    "            found.append(1)\n",
    "        else:\n",
    "            found.append(0)\n",
    "print (np.mean(found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding weights with vocab and zeros\n",
    "it_vocab = set()\n",
    "for sent in test_tweets:\n",
    "    for word in sent:\n",
    "        if word.text.lower() in embeddings['en']:\n",
    "            it_vocab.add(word.text.lower())\n",
    "        \n",
    "# replace embedding in model\n",
    "it_embeddings = np.zeros_like(clf.layers[0].get_weights()[0])\n",
    "it_word_2_index = {}\n",
    "for i, word in enumerate(it_vocab):\n",
    "    it_word_2_index[word] = i+1\n",
    "    it_embeddings[i+1] = embeddings['en'][word]\n",
    "\n",
    "    \n",
    "# encode sentences with new index\n",
    "clf_it = lstm_bilstm.load_model(os.path.join(base_dir, best_weights))\n",
    "clf_it.layers[0].set_weights([it_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sent(sent, word_idx_map, max_length=57):\n",
    "    encoded = np.array([word_idx_map[w.text.lower()] for w in sent if w.text.lower() in word_idx_map])\n",
    "    return encoded\n",
    "\n",
    "test_data = []\n",
    "for sent in test_tweets:\n",
    "    test_data.append(encode_sent(sent, it_word_2_index))\n",
    "test_data = lstm_bilstm.pad_sequences(test_data, max_length)\n",
    "    \n",
    "pred = clf_it.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4031600407747197\n"
     ]
    }
   ],
   "source": [
    "# true_labels = np.zeros_like(pred)\n",
    "# for i, l in enumerate(test_labels):\n",
    "#     true_labels[i][l] = 1.\n",
    "true_labels = keras.utils.to_categorical(test_labels)\n",
    "labels = sorted(set(dataset._ytrain.argmax(1)))\n",
    "mm = MyMetrics(true_labels, pred, labels=labels, average='micro')\n",
    "acc, precision, recall, micro_f1 = mm.get_scores()\n",
    "print(micro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "French translation results: F1 - 0.676  \n",
    "Italian translation results F1 - 0.427  \n",
    "\n",
    "French zero-shot: F1 - 0.669  \n",
    "Italian zero-shot: F1 - 0.513  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with data on different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6021"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset._Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading french data \n",
    "french_data = \"../../datasets/sentiment/french/French-Sentiment-Analysis-Dataset/tweets.csv\"\n",
    "labels = []\n",
    "tweets = []\n",
    "with open(french_data) as handle:\n",
    "    i = 0\n",
    "    for line in handle.readlines():\n",
    "        i += 1\n",
    "        if i == 1:\n",
    "            continue\n",
    "        try:\n",
    "            label = int(line[0])\n",
    "        except:\n",
    "            continue\n",
    "        text = line[2:]\n",
    "        tweets.append(text)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample 500 from each class of 0, 2 and 4\n",
    "fr_test_tweets = []\n",
    "fr_test_labels = []\n",
    "fr_train_tweets = []\n",
    "fr_train_labels = []\n",
    "zero = 0\n",
    "four = 0\n",
    "max_count = 3500\n",
    "max_count_test = 500\n",
    "for l, t in zip(labels, tweets):\n",
    "    if zero < max_count_test and l == 0:\n",
    "        zero += 1\n",
    "        fr_test_tweets.append(t)\n",
    "        fr_test_labels.append(l)\n",
    "    if four < max_count_test and l == 4:\n",
    "        four += 1\n",
    "        fr_test_tweets.append(t)\n",
    "        fr_test_labels.append(1)\n",
    "    if zero >= max_count_test and zero < max_count and l == 0:\n",
    "        zero += 1\n",
    "        fr_train_tweets.append(t)\n",
    "        fr_train_labels.append(l)\n",
    "    if four >= max_count_test and four < max_count and l == 4:\n",
    "        four += 1\n",
    "        fr_train_tweets.append(t)\n",
    "        fr_train_labels.append(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fr_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_nlp = spacy.load('fr')\n",
    "fr_train_tweets = [fr_nlp(t) for t in fr_train_tweets]\n",
    "fr_test_tweets = [fr_nlp(t) for t in fr_test_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the raw French model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"semeval\"\n",
    "bi = True\n",
    "dataset_raw = lstm_bilstm.Semeval_Dataset('../datasets/semeval',\n",
    "                                                None, rep=words,\n",
    "                                                one_hot=True)\n",
    "dataset = lstm_bilstm.Semeval_Dataset('../datasets/semeval',\n",
    "                                                None, rep=words,\n",
    "                                                one_hot=True)\n",
    "\n",
    "vecs = WordVecs('../../embeddings/wiki.multi.fr.vec', 'word2vec')\n",
    "dim = vecs.vector_size\n",
    "max_length = 57\n",
    "vocab = {}\n",
    "# for sent in list(dataset._Xtrain) + list(dataset._Xdev) + list(dataset._Xtest):\n",
    "#     if len(sent) > max_length:\n",
    "#         max_length = len(sent)\n",
    "#     for w in sent:\n",
    "#         if w not in vocab:\n",
    "#             vocab[w] = 1\n",
    "#         else:\n",
    "#             vocab[w] += 1\n",
    "\n",
    "for sent in fr_train_tweets:\n",
    "    for w in sent:\n",
    "        if w.text not in vocab:\n",
    "            vocab[w.text.lower()] = 1\n",
    "        else:\n",
    "            vocab[w.text.lower()] += 1\n",
    "            \n",
    "wordvecs = {}\n",
    "for w in vecs._w2idx.keys():\n",
    "    if w in vocab:\n",
    "        wordvecs[w] = vecs[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset train and val x,y\n",
    "train = False\n",
    "W, word_idx_map = lstm_bilstm.get_W(wordvecs)\n",
    "output_dim = 2\n",
    "name = 'semeval'\n",
    "bi=True\n",
    "dev_params_file = '../dev_params/300_bilstm.dev.txt'\n",
    "best_dim, best_dropout, best_epoch, best_f1 = lstm_bilstm.get_dev_params(name, dev_params_file, bi,\n",
    "                   dataset._Xtrain, dataset._ytrain, dataset._Xdev, dataset._ydev, wordvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "def encode_sent(sent, word_idx_map, max_length=57):\n",
    "    encoded = np.array([word_idx_map[w.text.lower()] for w in sent if w.text.lower() in word_idx_map])\n",
    "    return encoded\n",
    "\n",
    "fr_train_data = []\n",
    "for sent in fr_train_tweets:\n",
    "    fr_train_data.append(encode_sent(sent, word_idx_map))\n",
    "fr_train_data = lstm_bilstm.pad_sequences(fr_train_data, max_length)\n",
    "\n",
    "fr_test_data = []\n",
    "for sent in fr_test_tweets:\n",
    "    fr_test_data.append(encode_sent(sent, word_idx_map))\n",
    "fr_test_data = lstm_bilstm.pad_sequences(fr_test_data, max_length)\n",
    "\n",
    "fr_train_y = keras.utils.to_categorical(fr_train_labels)\n",
    "fr_test_y = keras.utils.to_categorical(fr_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/6\n",
      "6000/6000 [==============================] - 40s 7ms/step - loss: 0.6694 - acc: 0.6080 - val_loss: 0.6393 - val_acc: 0.6470\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.64700, saving model to models/bilstm/french_basic/run2/weights.001-0.6470.hdf5\n",
      "Epoch 2/6\n",
      "6000/6000 [==============================] - 38s 6ms/step - loss: 0.6227 - acc: 0.6610 - val_loss: 0.6390 - val_acc: 0.6330\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "Epoch 3/6\n",
      "6000/6000 [==============================] - 38s 6ms/step - loss: 0.6138 - acc: 0.6642 - val_loss: 0.6105 - val_acc: 0.6730\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.64700 to 0.67300, saving model to models/bilstm/french_basic/run2/weights.003-0.6730.hdf5\n",
      "Epoch 4/6\n",
      "6000/6000 [==============================] - 38s 6ms/step - loss: 0.6034 - acc: 0.6778 - val_loss: 0.6017 - val_acc: 0.6860\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.67300 to 0.68600, saving model to models/bilstm/french_basic/run2/weights.004-0.6860.hdf5\n",
      "Epoch 5/6\n",
      "6000/6000 [==============================] - 38s 6ms/step - loss: 0.5926 - acc: 0.6853 - val_loss: 0.6081 - val_acc: 0.6820\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 6/6\n",
      "6000/6000 [==============================] - 38s 6ms/step - loss: 0.5930 - acc: 0.6848 - val_loss: 0.5945 - val_acc: 0.6930\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.68600 to 0.69300, saving model to models/bilstm/french_basic/run2/weights.006-0.6930.hdf5\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "train=False\n",
    "new_name=\"french_basic\"\n",
    "run =2\n",
    "clf_french = lstm_bilstm.create_BiLSTM(wordvecs, best_dim, output_dim, best_dropout, weights=W, train=train)\n",
    "pathlib.Path('models/bilstm/' + new_name +'/run'+ str(run)).mkdir(parents=True, exist_ok=True)\n",
    "checkpoint = ModelCheckpoint('models/bilstm/' + new_name +'/run'+ str(run)+'/weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "\n",
    "h = clf_french.fit(fr_train_data, fr_train_y, validation_data=[fr_test_data, fr_test_y],\n",
    "            epochs=best_epoch, verbose=1, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.693\n"
     ]
    }
   ],
   "source": [
    "pred = clf_french.predict(fr_test_data)\n",
    "true_labels = np.zeros_like(pred)\n",
    "# for i, l in enumerate(fr_test_labels):\n",
    "#     true_labels[i][l] = 1.\n",
    "labels = sorted(set(dataset._ytrain.argmax(1)))\n",
    "mm = MyMetrics(fr_test_y, pred, labels=labels, average='micro')\n",
    "acc, precision, recall, micro_f1 = mm.get_scores()\n",
    "print(micro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
