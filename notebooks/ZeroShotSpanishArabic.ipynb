{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniter/Documents/jurafsky/multiling/sota_sentiment/.env3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import lstm_bilstm\n",
    "from Utils.WordVecs import *\n",
    "from Utils.MyMetrics import *\n",
    "from Utils.Datasets import *\n",
    "from Utils.Semeval_2013_Dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"semeval\"\n",
    "bi = True\n",
    "dataset_raw = lstm_bilstm.Semeval_Dataset('../datasets/semeval',\n",
    "                                                None, rep=words,\n",
    "                                                one_hot=True)\n",
    "dataset = lstm_bilstm.Semeval_Dataset('../datasets/semeval',\n",
    "                                                None, rep=words,\n",
    "                                                one_hot=True)\n",
    "\n",
    "vecs = WordVecs('../embeddings/wiki.multi.en.vec', 'word2vec')\n",
    "dim = vecs.vector_size\n",
    "max_length = 0\n",
    "vocab = {}\n",
    "for sent in list(dataset._Xtrain) + list(dataset._Xdev) + list(dataset._Xtest):\n",
    "    if len(sent) > max_length:\n",
    "        max_length = len(sent)\n",
    "    for w in sent:\n",
    "        if w not in vocab:\n",
    "            vocab[w] = 1\n",
    "        else:\n",
    "            vocab[w] += 1\n",
    "            \n",
    "wordvecs = {}\n",
    "for w in vecs._w2idx.keys():\n",
    "    if w in vocab:\n",
    "        wordvecs[w] = vecs[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_bilstm.add_unknown_words(wordvecs, vocab, min_df=1, dim=dim)\n",
    "W, word_idx_map = lstm_bilstm.get_W(wordvecs, dim=dim)\n",
    "\n",
    "dataset = lstm_bilstm.convert_dataset(dataset, word_idx_map, max_length)\n",
    "dev_params_file = '../dev_params/300_bilstm.dev.txt'\n",
    "best_dim, best_dropout, best_epoch, best_f1 = lstm_bilstm.get_dev_params(name, dev_params_file, bi,\n",
    "                   dataset._Xtrain, dataset._ytrain, dataset._Xdev, dataset._ydev, wordvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../models/bilstm/'+ name +'/run1'\n",
    "best_weights = \"weights.006-0.6337.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lstm_bilstm.load_model(os.path.join(base_dir, best_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2376/2376 [==============================] - 4s 2ms/step\n",
      "0.67003367003367\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(dataset._Xtest, verbose=1)\n",
    "labels = sorted(set(dataset._ytrain.argmax(1)))\n",
    "mm = MyMetrics(dataset._ytest, pred, labels=labels, average='micro')\n",
    "acc, precision, recall, micro_f1 = mm.get_scores()\n",
    "print(micro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero shot benchmark\n",
    "- Load data from other language (tweets)\n",
    "- Tokenize data\n",
    "- load embeddings for that language\n",
    "- run model on other language with no training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Spanish data \n",
    "spanish_data_file = \"../../datasets/sentiment/spanish/SEPLN-TASS15/DATA/general-tweets-train-tagged.xml\"\n",
    "e = xml.etree.ElementTree.parse(spanish_data_file).getroot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_data = []\n",
    "for tweet in e:\n",
    "    content = None\n",
    "    polarity = None\n",
    "    keep = False\n",
    "    for el in tweet:\n",
    "        if el.tag == 'content':\n",
    "            content = el.text\n",
    "        if el.tag == 'sentiments':\n",
    "            assert el[0].tag == 'polarity'\n",
    "            for entry in el[0]:\n",
    "                if entry.tag == 'type' and entry.text == 'AGREEMENT':\n",
    "                    keep = True\n",
    "                if entry.tag == 'value':\n",
    "                    polarity = entry.text\n",
    "    if keep and content is not None and polarity != 'None':\n",
    "        spanish_data.append((polarity, content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "import spacy\n",
    "es_nlp = spacy.load('es')\n",
    "test_tweets = [es_nlp(text) for label,text in spanish_data if text is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0 % complete ...\n",
      "50.0 % complete ...\n",
      "75.0 % complete ...\n",
      "100.0 % complete ...\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "es_file = '../../embeddings/wiki.multi.es.vec'\n",
    "lang_files = [es_file]\n",
    "\n",
    "embeddings = {}\n",
    "for lang_f in lang_files:\n",
    "    lang = lang_f[-6:-4]\n",
    "    embeddings[lang] = {}\n",
    "    with open(lang_f, 'r') as handle:\n",
    "        csv_file = csv.reader(handle, delimiter=' ', quotechar=\"|\")\n",
    "        i = 0\n",
    "        for row in csv_file:\n",
    "            if len(row) != 301:\n",
    "                continue\n",
    "            word = row[0]\n",
    "            vec = np.array(row[1:]).astype(np.float)\n",
    "            embeddings[lang][word] = vec\n",
    "            i += 1\n",
    "            if i % 50000 == 0:\n",
    "                print(i/2000., \"% complete ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7019815145656413\n"
     ]
    }
   ],
   "source": [
    "# Measure Coverage\n",
    "found = []\n",
    "for t in test_tweets:\n",
    "    for tok in t:\n",
    "        if tok.text in embeddings['es']:\n",
    "            found.append(1)\n",
    "        else:\n",
    "            found.append(0)\n",
    "print (np.mean(found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding weights with vocab and zeros\n",
    "es_vocab = set()\n",
    "for sent in test_tweets:\n",
    "    for word in sent:\n",
    "        if word.text in embeddings['es']:\n",
    "            es_vocab.add(word.text)\n",
    "        \n",
    "# replace embedding in model\n",
    "es_embeddings = np.zeros_like(clf.layers[0].get_weights()[0])\n",
    "es_word_2_index = {}\n",
    "for i, word in enumerate(es_vocab):\n",
    "    es_word_2_index[word] = i+1\n",
    "    es_embeddings[i+1] = embeddings['es'][word]\n",
    "\n",
    "    \n",
    "# encode sentences with new index\n",
    "clf_fr = lstm_bilstm.load_model(os.path.join(base_dir, best_weights))\n",
    "clf_fr.layers[0].set_weights([es_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sent(sent, word_idx_map, max_length=57):\n",
    "    encoded = np.array([word_idx_map[w.text] for w in sent if w.text in word_idx_map])\n",
    "    return encoded\n",
    "\n",
    "test_data = []\n",
    "for sent in test_tweets:\n",
    "    test_data.append(encode_sent(sent, es_word_2_index))\n",
    "test_data = lstm_bilstm.pad_sequences(test_data, max_length)\n",
    "    \n",
    "pred = clf_fr.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.zeros_like(pred)\n",
    "for i, (l, t) in enumerate([(l,t) for l,t in spanish_data if t is not None]):\n",
    "    pos = None\n",
    "    if \"P\" in l:\n",
    "        pos = 2\n",
    "    if \"N\" in l:\n",
    "        pos = 0\n",
    "    if l == 'NEU':\n",
    "        pos = 1\n",
    "    if pos == None:\n",
    "        print(\"Error\")\n",
    "        pos = 1\n",
    "    true_labels[i][pos] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2338622708365429\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = sorted(set(dataset._ytrain.argmax(1)))\n",
    "mm = MyMetrics(true_labels, pred, labels=labels, average='micro')\n",
    "acc, precision, recall, micro_f1 = mm.get_scores()\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6507258753202392\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(set(dataset._ytrain.argmax(1)))\n",
    "mm = MyMetrics(true_labels[:,[0,2]], pred[:,[0,2]], labels=labels, average='binary')\n",
    "acc, precision, recall, micro_f1 = mm.get_scores()\n",
    "print( micro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3270682483438607\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_labels = np.zeros_like(pred)\n",
    "for i, x in enumerate(pred):\n",
    "    if random.random() < .333:\n",
    "        pos = 0\n",
    "    elif random.random() > .333 and random.random() < .666:\n",
    "        pos = 1\n",
    "    else:\n",
    "        pos = 2\n",
    "    random_labels[i][pos] = 1.\n",
    "labels = sorted(set(dataset._ytrain.argmax(1)))\n",
    "mm = MyMetrics(random_labels, pred, labels=labels, average='micro')\n",
    "acc, precision, recall, micro_f1 = mm.get_scores()\n",
    "print(micro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392\n",
      "4855\n",
      "1244\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(np.argmax(pred, axis=1) == 0))\n",
    "print(np.sum(np.argmax(pred, axis=1) == 1))\n",
    "print(np.sum(np.argmax(pred, axis=1) == 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3464\n",
      "313\n",
      "2714\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(np.argmax(true_labels, axis=1) == 0))\n",
    "print(np.sum(np.argmax(true_labels, axis=1) == 1))\n",
    "print(np.sum(np.argmax(true_labels, axis=1) == 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading french data \n",
    "# 1 = positive, 0 = negative\n",
    "arabic_data1 = \"../../datasets/sentiment/arabic/ar-embeddings/datasets/tweets/ASTD.csv\"\n",
    "arabic_data2 = \"../../datasets/sentiment/arabic/ar-embeddings/datasets/tweets/ArTwitter.csv\"\n",
    "ar_labels = []\n",
    "tweets = []\n",
    "with open(arabic_data1) as handle:\n",
    "    i = 0\n",
    "    for line in handle.readlines():\n",
    "        i += 1\n",
    "        if i == 1:\n",
    "            continue\n",
    "        try:\n",
    "            label = int(line[0])\n",
    "        except:\n",
    "            continue\n",
    "        text = line[2:]\n",
    "        tweets.append(text)\n",
    "        ar_labels.append(label)\n",
    "        \n",
    "with open(arabic_data2) as handle:\n",
    "    i = 0\n",
    "    for line in handle.readlines():\n",
    "        i += 1\n",
    "        if i == 1:\n",
    "            continue\n",
    "        try:\n",
    "            label = int(line[0])\n",
    "        except:\n",
    "            continue\n",
    "        text = line[2:]\n",
    "        tweets.append(text)\n",
    "        ar_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarabic.araby as araby\n",
    "test_tweets = [araby.tokenize(t) for t in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0 % complete ...\n",
      "50.0 % complete ...\n",
      "75.0 % complete ...\n",
      "100.0 % complete ...\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "ar_file = '../../embeddings/wiki.multi.ar.vec'\n",
    "lang_files = [ar_file]\n",
    "\n",
    "embeddings = {}\n",
    "for lang_f in lang_files:\n",
    "    lang = lang_f[-6:-4]\n",
    "    embeddings[lang] = {}\n",
    "    with open(lang_f, 'r') as handle:\n",
    "        csv_file = csv.reader(handle, delimiter=' ', quotechar=\"|\")\n",
    "        i = 0\n",
    "        for row in csv_file:\n",
    "            if len(row) != 301:\n",
    "                continue\n",
    "            word = row[0]\n",
    "            vec = np.array(row[1:]).astype(np.float)\n",
    "            embeddings[lang][word] = vec\n",
    "            i += 1\n",
    "            if i % 50000 == 0:\n",
    "                print(i/2000., \"% complete ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8076509605129535\n"
     ]
    }
   ],
   "source": [
    "# Unfortunately we only have embeddings for 76% of the tokens\n",
    "found = []\n",
    "for t in tweets:\n",
    "    for tok in t:\n",
    "        if tok in embeddings['ar']:\n",
    "            found.append(1)\n",
    "        else:\n",
    "            found.append(0)\n",
    "print (np.mean(found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding weights with vocab and zeros\n",
    "ar_vocab = set()\n",
    "for sent in tweets:\n",
    "    for word in sent:\n",
    "        if word in embeddings['ar']:\n",
    "            ar_vocab.add(word)\n",
    "        \n",
    "# replace embedding in model\n",
    "ar_embeddings = np.zeros_like(clf.layers[0].get_weights()[0])\n",
    "ar_word_2_index = {}\n",
    "for i, word in enumerate(ar_vocab):\n",
    "    ar_word_2_index[word] = i+1\n",
    "    ar_embeddings[i+1] = embeddings['ar'][word]\n",
    "\n",
    "    \n",
    "# encode sentences with new index\n",
    "clf_ar = lstm_bilstm.load_model(os.path.join(base_dir, best_weights))\n",
    "clf_ar.layers[0].set_weights([ar_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sent(sent, word_idx_map, max_length=57):\n",
    "    encoded = np.array([word_idx_map[w] for w in sent if w in word_idx_map])\n",
    "    return encoded\n",
    "\n",
    "test_data = []\n",
    "for sent in tweets:\n",
    "    test_data.append(encode_sent(sent, ar_word_2_index))\n",
    "test_data = lstm_bilstm.pad_sequences(test_data, max_length)\n",
    "    \n",
    "pred = clf_ar.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.zeros_like(pred)\n",
    "for i, l in enumerate(ar_labels):\n",
    "    if l == 1:\n",
    "        pos = 2\n",
    "    if l == 0:\n",
    "        pos = 0\n",
    "    true_labels[i][pos] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.500423131170663 0.5004236091499576 0.9988726042841037 0.6667920978363124\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(set(dataset._ytrain.argmax(1)))\n",
    "mm = MyMetrics(true_labels[:,[0,2]], pred[:,[0,2]], labels=labels, average='binary')\n",
    "acc, precision, recall, micro_f1 = mm.get_scores()\n",
    "print(acc, precision, recall, micro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3541"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.argmax(pred[:,[0,2]], axis=1)==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
