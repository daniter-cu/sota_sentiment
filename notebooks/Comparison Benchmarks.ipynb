{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Benchmarks\n",
    "We will compare the previous results to:  \n",
    "(1) Training on the original data  \n",
    "(2) Evaluating the translations  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate translations for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'key':'trnsl.1.1.20180425T012837Z.34aff5ff1fbca7f9.449ba9cd74e4c4e829e77e60507b69d30b425d56',\n",
    "    'text':['hello world', 'this is great', 'one more'],\n",
    "    'lang':'es'\n",
    "}\n",
    "r = requests.post('https://translate.yandex.net/api/v1.5/tr.json/translate', data=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hola mundo', 'esta es una gran', 'uno de los m√°s']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.json()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading french data \n",
    "french_data = \"../../datasets/sentiment/french/French-Sentiment-Analysis-Dataset/tweets.csv\"\n",
    "labels = []\n",
    "tweets = []\n",
    "with open(french_data) as handle:\n",
    "    i = 0\n",
    "    for line in handle.readlines():\n",
    "        i += 1\n",
    "        if i == 1:\n",
    "            continue\n",
    "        try:\n",
    "            label = int(line[0])\n",
    "        except:\n",
    "            continue\n",
    "        text = line[2:]\n",
    "        tweets.append(text)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample 500 from each class of 0, 2 and 4\n",
    "test_tweets = []\n",
    "test_labels = []\n",
    "zero = 0\n",
    "two = 0\n",
    "four = 0\n",
    "max_count = 500\n",
    "for l, t in zip(labels, tweets):\n",
    "    if zero < max_count and l == 0:\n",
    "        zero += 1\n",
    "        test_tweets.append(t)\n",
    "        test_labels.append(l)\n",
    "    if two < max_count and l == 2:\n",
    "        two += 1\n",
    "        test_tweets.append(t)\n",
    "        test_labels.append(l)\n",
    "    if four < max_count and l == 4:\n",
    "        four += 1\n",
    "        test_tweets.append(t)\n",
    "        test_labels.append(l)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.839"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(t) for t in test_tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading french data \n",
    "italian_data = \"../../datasets/sentiment/italian/test_set_sentipolc16_gold2000.csv\"\n",
    "test_labels = []\n",
    "tweets = []\n",
    "with open(italian_data) as handle:\n",
    "    i = 0\n",
    "    reader = csv.reader(handle)\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        text = row[8]\n",
    "        label_p = int(row[2])\n",
    "        label_n = int(row[3])\n",
    "        label = 1\n",
    "        if label_p == 1:\n",
    "            label = 2\n",
    "        if label_n == 1:\n",
    "            label = 0\n",
    "        if label_p + label_n == 2:\n",
    "            continue\n",
    "        tweets.append(text)\n",
    "        test_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109.70438328236493"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(t) for t in tweets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and store all translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translation(text_list, lang):\n",
    "    params = {\n",
    "        'key':'trnsl.1.1.20180425T012837Z.34aff5ff1fbca7f9.449ba9cd74e4c4e829e77e60507b69d30b425d56',\n",
    "        'text':text_list,\n",
    "        'lang':lang\n",
    "    }\n",
    "    r = requests.post('https://translate.yandex.net/api/v1.5/tr.json/translate', data=params)\n",
    "    return r.json()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# req_size = 50\n",
    "# i = 0 \n",
    "# french_translations = []\n",
    "# while i < len(test_tweets):\n",
    "#     end = i + req_size\n",
    "#     if end > len(test_tweets):\n",
    "#         end = len(test_tweets)\n",
    "#     trans = get_translation(test_tweets[i:end], 'fr-en')\n",
    "#     french_translations += trans\n",
    "#     i += req_size\n",
    "\n",
    "\n",
    "# req_size = 50\n",
    "# i = 0 \n",
    "# italian_translations = []\n",
    "# while i < len(tweets):\n",
    "#     end = i + req_size\n",
    "#     if end > len(tweets):\n",
    "#         end = len(tweets)\n",
    "#     trans = get_translation(tweets[i:end], 'it-en')\n",
    "#     italian_translations += trans\n",
    "#     i += req_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"french_trans.pkl\", 'wb') as handle:\n",
    "#     pickle.dump(french_translations, handle)\n",
    "    \n",
    "# with open(\"it_trans.pkl\", 'wb') as handle:\n",
    "#     pickle.dump(italian_translations, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(french_translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1962"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(italian_translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You do not see the hour that is here, at least, I will miss you more often and I will finally be able to be with him, in the face of the one who has friendzonato.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italian_translations[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# French translation eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "test_tweets = [nlp(t) for t in french_translations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0 % complete ...\n",
      "50.0 % complete ...\n",
      "75.0 % complete ...\n",
      "100.0 % complete ...\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "#fr_file = '../../embeddings/wiki.multi.fr.vec'\n",
    "#it_file = '../../embeddings/wiki.multi.it.vec'\n",
    "en_file = '../../embeddings/wiki.multi.en.vec'\n",
    "lang_files = [en_file]\n",
    "\n",
    "embeddings = {}\n",
    "for lang_f in lang_files:\n",
    "    lang = lang_f[-6:-4]\n",
    "    embeddings[lang] = {}\n",
    "    with open(lang_f, 'r') as handle:\n",
    "        csv_file = csv.reader(handle, delimiter=' ', quotechar=\"|\")\n",
    "        i = 0\n",
    "        for row in csv_file:\n",
    "            if len(row) != 301:\n",
    "                continue\n",
    "            word = row[0]\n",
    "            vec = np.array(row[1:]).astype(np.float)\n",
    "            embeddings[lang][word] = vec\n",
    "            i += 1\n",
    "            if i % 50000 == 0:\n",
    "                print(i/2000., \"% complete ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7635722812802\n"
     ]
    }
   ],
   "source": [
    "# Unfortunately we only have embeddings for 76% of the tokens\n",
    "found = []\n",
    "for t in test_tweets:\n",
    "    for tok in t:\n",
    "        if tok.text in embeddings['en']:\n",
    "            found.append(1)\n",
    "        else:\n",
    "            found.append(0)\n",
    "print (np.mean(found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lstm_bilstm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"semeval\"\n",
    "base_dir = '../models/bilstm/'+ name +'/run1'\n",
    "best_weights = \"weights.006-0.6337.hdf5\"\n",
    "clf = lstm_bilstm.load_model(os.path.join(base_dir, best_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding weights with vocab and zeros\n",
    "vocab = set()\n",
    "for sent in test_tweets:\n",
    "    for word in sent:\n",
    "        if word.text in embeddings['en']:\n",
    "            vocab.add(word.text)\n",
    "        \n",
    "# replace embedding in model\n",
    "en_embeddings = np.zeros_like(clf.layers[0].get_weights()[0])\n",
    "word_2_index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word_2_index[word] = i+1\n",
    "    en_embeddings[i+1] = embeddings['en'][word]\n",
    "\n",
    "    \n",
    "# encode sentences with new index\n",
    "clf_fr = lstm_bilstm.load_model(os.path.join(base_dir, best_weights))\n",
    "clf_fr.layers[0].set_weights([en_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.WordVecs import *\n",
    "from Utils.MyMetrics import *\n",
    "from Utils.Datasets import *\n",
    "from Utils.Semeval_2013_Dataset import *\n",
    "dataset = lstm_bilstm.Semeval_Dataset('../datasets/semeval',\n",
    "                                                None, rep=words,\n",
    "                                                one_hot=True)\n",
    "max_length = 0\n",
    "for sent in list(dataset._Xtrain) + list(dataset._Xdev) + list(dataset._Xtest):\n",
    "    if len(sent) > max_length:\n",
    "        max_length = len(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sent(sent, word_idx_map, max_length=57):\n",
    "    encoded = np.array([word_idx_map[w.text] for w in sent if w.text in word_idx_map])\n",
    "    return encoded\n",
    "\n",
    "test_data = []\n",
    "for sent in test_tweets:\n",
    "    test_data.append(encode_sent(sent, word_2_index))\n",
    "test_data = lstm_bilstm.pad_sequences(test_data, max_length)\n",
    "    \n",
    "pred = clf_fr.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.zeros_like(pred)\n",
    "for i, l in enumerate(test_labels):\n",
    "    pos = int(l/2)\n",
    "    true_labels[i][pos] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.373\n"
     ]
    }
   ],
   "source": [
    "# labels = sorted(set(dataset._ytrain.argmax(1)))\n",
    "# mm = MyMetrics(true_labels, pred, labels=labels, average='micro')\n",
    "# acc, precision, recall, micro_f1 = mm.get_scores()\n",
    "# print(micro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.616 0.5845481049562682 0.802 0.6762225969645869\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(set(dataset._ytrain.argmax(1)))\n",
    "mm = MyMetrics(true_labels[:,[0,2]], pred[:,[0,2]], labels=labels, average='binary')\n",
    "acc, precision, recall, micro_f1 = mm.get_scores()\n",
    "print(acc, precision, recall, micro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading french data \n",
    "italian_data = \"../../datasets/sentiment/italian/test_set_sentipolc16_gold2000.csv\"\n",
    "test_labels = []\n",
    "tweets = []\n",
    "with open(italian_data) as handle:\n",
    "    i = 0\n",
    "    reader = csv.reader(handle)\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        text = row[8]\n",
    "        label_p = int(row[2])\n",
    "        label_n = int(row[3])\n",
    "        label = 1\n",
    "        if label_p == 1:\n",
    "            label = 2\n",
    "        if label_n == 1:\n",
    "            label = 0\n",
    "        if label_p + label_n == 2:\n",
    "            continue\n",
    "        tweets.append(text)\n",
    "        test_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "it_nlp = spacy.load('en')\n",
    "test_tweets = [it_nlp(t) for t in italian_translations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.723447680351676\n"
     ]
    }
   ],
   "source": [
    "found = []\n",
    "for t in test_tweets:\n",
    "    for tok in t:\n",
    "        if tok.text in embeddings['en']:\n",
    "            found.append(1)\n",
    "        else:\n",
    "            found.append(0)\n",
    "print (np.mean(found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding weights with vocab and zeros\n",
    "it_vocab = set()\n",
    "for sent in test_tweets:\n",
    "    for word in sent:\n",
    "        if word.text in embeddings['en']:\n",
    "            it_vocab.add(word.text)\n",
    "        \n",
    "# replace embedding in model\n",
    "it_embeddings = np.zeros_like(clf.layers[0].get_weights()[0])\n",
    "it_word_2_index = {}\n",
    "for i, word in enumerate(it_vocab):\n",
    "    it_word_2_index[word] = i+1\n",
    "    it_embeddings[i+1] = embeddings['en'][word]\n",
    "\n",
    "    \n",
    "# encode sentences with new index\n",
    "clf_it = lstm_bilstm.load_model(os.path.join(base_dir, best_weights))\n",
    "clf_it.layers[0].set_weights([it_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sent(sent, word_idx_map, max_length=57):\n",
    "    encoded = np.array([word_idx_map[w.text] for w in sent if w.text in word_idx_map])\n",
    "    return encoded\n",
    "\n",
    "test_data = []\n",
    "for sent in test_tweets:\n",
    "    test_data.append(encode_sent(sent, it_word_2_index))\n",
    "test_data = lstm_bilstm.pad_sequences(test_data, max_length)\n",
    "    \n",
    "pred = clf_it.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42660550458715596\n"
     ]
    }
   ],
   "source": [
    "true_labels = np.zeros_like(pred)\n",
    "for i, l in enumerate(test_labels):\n",
    "    true_labels[i][l] = 1.\n",
    "labels = sorted(set(dataset._ytrain.argmax(1)))\n",
    "mm = MyMetrics(true_labels, pred, labels=labels, average='micro')\n",
    "acc, precision, recall, micro_f1 = mm.get_scores()\n",
    "print(micro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "French translation results: F1 - 0.676  \n",
    "Italian translation results F1 - 0.427  \n",
    "\n",
    "French zero-shot: F1 - 0.669  \n",
    "Italian zero-shot: F1 - 0.513  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
