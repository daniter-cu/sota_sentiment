{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lstm_bilstm\n",
    "from Utils.WordVecs import *\n",
    "from Utils.MyMetrics import *\n",
    "from Utils.Datasets import *\n",
    "from Utils.Semeval_2013_Dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"sst_binary\"\n",
    "bi = True\n",
    "dataset_raw = lstm_bilstm.Stanford_Sentiment_Dataset('datasets/stanford_sentanalysis',\n",
    "                                            None,\n",
    "                                            one_hot=True,\n",
    "                                            binary=True,\n",
    "                                            rep=words)\n",
    "dataset = lstm_bilstm.Stanford_Sentiment_Dataset('datasets/stanford_sentanalysis',\n",
    "                                            None,\n",
    "                                            one_hot=True,\n",
    "                                            binary=True,\n",
    "                                            rep=words)\n",
    "\n",
    "vecs = WordVecs('embeddings/wiki.multi.en.vec', 'word2vec')\n",
    "dim = vecs.vector_size\n",
    "max_length = 0\n",
    "vocab = {}\n",
    "for sent in list(dataset._Xtrain) + list(dataset._Xdev) + list(dataset._Xtest):\n",
    "    if len(sent) > max_length:\n",
    "        max_length = len(sent)\n",
    "    for w in sent:\n",
    "        if w not in vocab:\n",
    "            vocab[w] = 1\n",
    "        else:\n",
    "            vocab[w] += 1\n",
    "            \n",
    "wordvecs = {}\n",
    "for w in vecs._w2idx.keys():\n",
    "    if w in vocab:\n",
    "        wordvecs[w] = vecs[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_bilstm.add_unknown_words(wordvecs, vocab, min_df=1, dim=dim)\n",
    "W, word_idx_map = lstm_bilstm.get_W(wordvecs, dim=dim)\n",
    "\n",
    "dataset = lstm_bilstm.convert_dataset(dataset, word_idx_map, max_length)\n",
    "dev_params_file = 'dev_params/300_bilstm.dev.txt'\n",
    "best_dim, best_dropout, best_epoch, best_f1 = lstm_bilstm.get_dev_params(name, dev_params_file, bi,\n",
    "                   dataset._Xtrain, dataset._ytrain, dataset._Xdev, dataset._ydev, wordvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'models/bilstm/'+ name +'/run1'\n",
    "best_weights = \"weights.005-0.8062.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lstm_bilstm.load_model(os.path.join(base_dir, best_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(dataset._Xtest, verbose=1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't need this one, it just converts above to hard label\n",
    "classes = clf.predict_classes(dataset._Xtest, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(set(dataset._ytrain.argmax(1)))\n",
    "mm = MyMetrics(dataset._ytest, pred, labels=labels, average='binary')\n",
    "acc, precision, recall, micro_f1 = mm.get_scores()\n",
    "print(micro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sent(sent, word_idx_map, max_length=57):\n",
    "    encoded = np.array([word_idx_map[w] for w in sent])\n",
    "    return lstm_bilstm.pad_sequences([encoded], max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(encode_sent(dataset_raw._Xtest[0], word_idx_map)[0], dataset._Xtest[0]), \"encode sentence not functioning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = encode_sent(\"really great movie loved it\".lower().split(), word_idx_map)\n",
    "clf.predict(test_sent, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Random input from other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_file = '../embeddings/wiki.multi.en.vec'\n",
    "heb_file = '../embeddings/wiki.multi.he.vec'\n",
    "rus_file = '../embeddings/wiki.multi.ru.vec'\n",
    "lang_files = [en_file, heb_file, rus_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "for lang_f in lang_files:\n",
    "    lang = lang_f[-6:-4]\n",
    "    embeddings[lang] = {}\n",
    "    with open(lang_f, 'r') as handle:\n",
    "        csv_file = csv.reader(handle, delimiter=' ', quotechar=\"|\")\n",
    "        i = 0\n",
    "        for row in csv_file:\n",
    "            if len(row) != 301:\n",
    "                continue\n",
    "            word = row[0]\n",
    "            vec = np.array(row[1:]).astype(np.float)\n",
    "            embeddings[lang][word] = vec\n",
    "            i += 1\n",
    "            if i % 50000 == 0:\n",
    "                print(i/2000., \"% complete ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_emb_weights = clf.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = np.array(saved_emb_weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test new languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocabulary\n",
    "he_test_sents = [\n",
    "    \"ה סרט היה משעמם ו בזבוז זמן\".split(),\n",
    "    \"שנא תי את ה סרט ה זה רוצה את ה כסף שלי חזרה\".split(),\n",
    "    \"זה היה מדהים אני רוצה לראות אותו עוד אלף פעמים\".split(),\n",
    "    \"פשוט תענוג כל ה כבוד ל שחקנים\".split(),\n",
    "    \"רע\".split(),\n",
    "    \"משעמם\".split(),\n",
    "    \"טוב מאוד\".split(),\n",
    "    \"מדהים\".split()\n",
    "]\n",
    "ru_test_sents = [\n",
    "    \"фильм был тупой и не интересный\".split(),\n",
    "    \"ужасный фильм совсем не любил\".split(),\n",
    "    \"замечательный фильм очень понравилось\".split(),\n",
    "    \"я очень любил фильм было интересно и весело\".split(),\n",
    "    \"плохо\".split(),\n",
    "    \"скучно\".split(),\n",
    "    \"очень хорошо\".split(),\n",
    "    \"замечательно\".split()\n",
    "]\n",
    "# create embedding weights with vocab and zeros\n",
    "he_vocab = set()\n",
    "for sent in he_test_sents:\n",
    "    for word in sent:\n",
    "        assert word in embeddings['he'], \"Didn't find %s\"% word\n",
    "        he_vocab.add(word)\n",
    "ru_vocab = set()\n",
    "for sent in ru_test_sents:\n",
    "    for word in sent:\n",
    "        assert word in embeddings['ru'], \"Didn't find %s\"% word\n",
    "        ru_vocab.add(word)\n",
    "        \n",
    "# replace embedding in model\n",
    "ru_embeddings = np.zeros_like(saved_emb_weights[0])\n",
    "he_embeddings = np.zeros_like(saved_emb_weights[0])\n",
    "ru_word_2_index = {}\n",
    "for i, word in enumerate(ru_vocab):\n",
    "    ru_word_2_index[word] = i\n",
    "    ru_embeddings[0] = embeddings['ru'][word]\n",
    "he_word_2_index = {}\n",
    "for i, word in enumerate(he_vocab):\n",
    "    he_word_2_index[word] = i\n",
    "    he_embeddings[0] = embeddings['he'][word]\n",
    "    \n",
    "# encode sentences with new index\n",
    "clf_ru = lstm_bilstm.load_model(os.path.join(base_dir, best_weights))\n",
    "clf_ru.layers[0].set_weights([ru_embeddings])\n",
    "clf_he = lstm_bilstm.load_model(os.path.join(base_dir, best_weights))\n",
    "clf_ru.layers[0].set_weights([he_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in ru_test_sents:\n",
    "    test_sent = encode_sent(sent, ru_word_2_index)\n",
    "    pred = clf_ru.predict(test_sent, verbose=1)\n",
    "    print(pred)\n",
    "    print(sent)\n",
    "    print(\"#\"*20)\n",
    "    \n",
    "for sent in he_test_sents:\n",
    "    test_sent = encode_sent(sent, he_word_2_index)\n",
    "    pred = clf_he.predict(test_sent, verbose=1)\n",
    "    print(pred)\n",
    "    print(sent)\n",
    "    print(\"#\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = encode_sent(\"film was dumb and not interesting\".lower().split(), word_idx_map)\n",
    "print(clf.predict(test_sent, verbose=1))\n",
    "\n",
    "test_sent = encode_sent(\"awful film completely no love\".lower().split(), word_idx_map)\n",
    "print(clf.predict(test_sent, verbose=1))\n",
    "\n",
    "test_sent = encode_sent(\"wonderful film very liked\".lower().split(), word_idx_map)\n",
    "print(clf.predict(test_sent, verbose=1))\n",
    "\n",
    "test_sent = encode_sent(\"i much loved film was interesting and fun\".lower().split(), word_idx_map)\n",
    "print(clf.predict(test_sent, verbose=1))\n",
    "\n",
    "test_sent = encode_sent(\"the movie was boring and waste time\".lower().split(), word_idx_map)\n",
    "print(clf.predict(test_sent, verbose=1))\n",
    "\n",
    "test_sent = encode_sent(\"i hated this movie want my money back\".lower().split(), word_idx_map)\n",
    "print(clf.predict(test_sent, verbose=1))\n",
    "\n",
    "test_sent = encode_sent(\"this was wonderful i want to see it another thousand times\".lower().split(), word_idx_map)\n",
    "print(clf.predict(test_sent, verbose=1))\n",
    "\n",
    "test_sent = encode_sent(\"simply pleasure all the respect to actors\".lower().split(), word_idx_map)\n",
    "print(clf.predict(test_sent, verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = encode_sent(\"bad\".lower().split(), word_idx_map)\n",
    "print(clf.predict(test_sent, verbose=1))\n",
    "\n",
    "test_sent = encode_sent(\"boring\".lower().split(), word_idx_map)\n",
    "print(clf.predict(test_sent, verbose=1))\n",
    "\n",
    "test_sent = encode_sent(\"very good\".lower().split(), word_idx_map)\n",
    "print(clf.predict(test_sent, verbose=1))\n",
    "\n",
    "test_sent = encode_sent(\"amazing\".lower().split(), word_idx_map)\n",
    "print(clf.predict(test_sent, verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
